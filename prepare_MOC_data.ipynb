{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDDIT_TIMELINE_DIR = \"/import/nlp/datasets/MoC/CLPsych_Reddit/\"\n",
    "REDDIT_HASH_DIR = \"/import/nlp/datasets/MoC/CLPsych_Reddit/hashing/\"\n",
    "REDDIT_PICKLE_DIR = \"/import/nlp/datasets/Reddit/users/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit user name\n",
    "USER = \"spectralconfetti\"\n",
    "\n",
    "# open pickle file\n",
    "with open(os.path.join(REDDIT_PICKLE_DIR, USER + \".p\"), \"rb\") as f:\n",
    "    reddit_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hashtable \n",
    "with open(os.path.join(REDDIT_HASH_DIR, \"reddit_new_hashed.json\"), \"r\") as f:\n",
    "    reddit_hash = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['50819d54b1', 'f55ec24987', 'e71b4656e6', '6d8bcf0531']\n"
     ]
    }
   ],
   "source": [
    "# find all timelines for user in hash table\n",
    "user_timeline_ids = []\n",
    "\n",
    "for hashkey in reddit_hash.keys():\n",
    "    if USER in hashkey:\n",
    "        user_timeline_ids.append(reddit_hash[hashkey])\n",
    "\n",
    "print(user_timeline_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/import/nlp/datasets/MoC/CLPsych_Reddit/test/50819d54b1.tsv\n",
      "/import/nlp/datasets/MoC/CLPsych_Reddit/train/50819d54b1.tsv\n",
      "Found file /import/nlp/datasets/MoC/CLPsych_Reddit/train/50819d54b1.tsv\n",
      "/import/nlp/datasets/MoC/CLPsych_Reddit/test/f55ec24987.tsv\n",
      "/import/nlp/datasets/MoC/CLPsych_Reddit/train/f55ec24987.tsv\n",
      "Found file /import/nlp/datasets/MoC/CLPsych_Reddit/train/f55ec24987.tsv\n",
      "/import/nlp/datasets/MoC/CLPsych_Reddit/test/e71b4656e6.tsv\n",
      "/import/nlp/datasets/MoC/CLPsych_Reddit/train/e71b4656e6.tsv\n",
      "Found file /import/nlp/datasets/MoC/CLPsych_Reddit/train/e71b4656e6.tsv\n",
      "/import/nlp/datasets/MoC/CLPsych_Reddit/test/6d8bcf0531.tsv\n",
      "/import/nlp/datasets/MoC/CLPsych_Reddit/train/6d8bcf0531.tsv\n"
     ]
    }
   ],
   "source": [
    "# load tsv files with timelines\n",
    "user_timeline_paths = []\n",
    "for timeline_id in user_timeline_ids:\n",
    "\n",
    "    # find where the file is located\n",
    "    for subfolder in [\"test\", \"train\"]:\n",
    "        file_path = os.path.join(REDDIT_TIMELINE_DIR, subfolder, timeline_id + \".tsv\")\n",
    "        print(file_path)\n",
    "        if os.path.exists(file_path):\n",
    "            user_timeline_paths.append(file_path)\n",
    "            print(f\"Found file {file_path}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to open tsv file\n",
    "user_timeline_files = []\n",
    "for user_timeline_path in user_timeline_paths:\n",
    "    df = pd.read_csv(user_timeline_path, sep='\\t')\n",
    "    user_timeline_files.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the post ids from the original posts are not the same as the post ids in the timeline files\n",
    "# so we have to do text matching and then attach the original id to the timeline\n",
    "\n",
    "# get all titles from the original posts\n",
    "og_titles = {}\n",
    "for post in reddit_data:\n",
    "    og_titles[post[\"title\"]] = post[\"id\"]\n",
    "\n",
    "for timeline_df in user_timeline_files:\n",
    "    og_ids = []\n",
    "    # iteratie through titles\n",
    "    for i, title in enumerate(timeline_df[\"title\"]):\n",
    "        \n",
    "        # iterate through the original titles\n",
    "        \n",
    "        for og_title in og_titles.keys():\n",
    "            # if the title is in the original titles\n",
    "            if title in og_title:\n",
    "                og_ids.append(og_titles[og_title])\n",
    "                break\n",
    "        \n",
    "    # add og_ids to the timeline_df as new column\n",
    "    timeline_df[\"ogpostid\"] = og_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with all posts where the keys or the post id:\n",
    "\n",
    "# iterate through the original posts\n",
    "og_posts = {}\n",
    "for post in reddit_data:\n",
    "    og_posts[post[\"id\"]] = {\n",
    "        \"title\": post[\"title\"],\n",
    "        \"body\": post.get(\"selftext\", \"\"),\n",
    "        \"created_utc\": post[\"created_utc\"],\n",
    "        \"label\": [0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timelines to dictionaries with a summary and post ids\n",
    "# also add labels to original data\n",
    "\n",
    "timeline_dict = {}\n",
    "\n",
    "for timeline_df in user_timeline_files:\n",
    "    # get og_ids \n",
    "    og_ids = timeline_df[\"ogpostid\"].tolist()\n",
    "\n",
    "    # create key that indicates the range of posts\n",
    "    key = f\"{og_ids[0]}-{og_ids[-1]}\"\n",
    "\n",
    "    timeline_dict[key] = {\n",
    "        \"timeline_of_interest\": True,\n",
    "        \"posts\": og_ids,\n",
    "        \"summary\": \"\",\n",
    "    }\n",
    "\n",
    "    for row in timeline_df.iterrows():\n",
    "        # get the post id\n",
    "        post_id = row[1][\"ogpostid\"]\n",
    "\n",
    "        # get the label\n",
    "        label = row[1][\"label\"]\n",
    "\n",
    "        # add the label to the original post\n",
    "        og_posts[post_id][\"label\"] = [label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data as json\n",
    "with open(f\"public/data/{USER}_posts.json\", \"w\") as f:\n",
    "    json.dump(og_posts, f, indent=4)\n",
    "\n",
    "with open(f\"public/data/{USER}_timelines.json\", \"w\") as f:\n",
    "    json.dump(timeline_dict, f, indent=4)\n",
    "\n",
    "# open user id json and add the new id\n",
    "with open(f\"src/assets/user_ids.json\", \"r\") as f:\n",
    "    user_ids = json.load(f)\n",
    "\n",
    "# add the new user id to the json\n",
    "user_ids[\"ids\"].append(USER)\n",
    "# save the json\n",
    "with open(f\"src/assets/user_ids.json\", \"w\") as f:\n",
    "    json.dump(user_ids, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
